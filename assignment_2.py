# -*- coding: utf-8 -*-
"""assignment-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CuCay_soPh_Tx9N3-bnNPcWv2uVpA5sp
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
import random
from keras.models import Sequential, Model
from keras.layers import Dense
from keras.optimizers import Adam
from tensorflow.keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.cluster import MiniBatchKMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score,\
silhouette_score, adjusted_rand_score

def data_preprocessing(arr):
    '''
    A function that normalizes the pixel values
    and reshapes the images into a flat array.

    INPUT
    -----
    arr: numpy.ndarray

    OUTPUT
    ------
    arr: numpy.array (transformed data into a flatten 1-D array
                     with normalized values)

    '''

    # Normalize pixel values and assigning "float32" data type
    arr = (arr / 255.0).astype(np.float32)

    # Reshape multi-dimensional array into an 1-D array of nxn = n^2 elements
    arr = arr.reshape(arr.shape[0], -1)

    return arr

def index_generator(arr1, arr2):
    '''
    Storing the indices of randomly chosen examples

    INPUT
    -----
    arr1: np.array (features)
    arr2: np.array (labels)

    OUTPUT
    ------
    indices: dict (indices of randomly chosen examples)

    '''
    # Indices of chosen examples
    indices = dict()
    for value in np.unique(arr2):
        # Random chosen element
        rand_num = np.random.choice(np.where(arr2 == value)[0])
        indices.update({value: rand_num})

    return indices

def visuals_1(arr1, arr2, index_vals, title):
    '''
    Visualizing images corresponding to different
    labels.

    INPUT
    -----
    arr1: np.array (features)
    arr2: np.array (labels)
    index_vals: dictionary of indices
    title: str (title of plot)

    OUTPUT
    ------
    plot (not returned)

    '''
    # Creating subplots
    fig, axes = plt.subplots(1, 10, figsize=(10, 5))

    for key in index_vals.keys():
        # Plotting a random element corresponding to "value" class
        axes[key].imshow(arr1[index_vals[key]].reshape(28, 28), plt.cm.binary)
        axes[key].set_title(labels[key])
        axes[key].axis('off')

    plt.suptitle(title, fontsize = 16)
    plt.subplots_adjust(top = 1.4)
    plt.show()

def dim_reduction(function, method, train_data, test_data):
    '''
    Applying dimensionality reduction techniques to
    the data.

    INPUT
    -----
    function: dim. reduction function from scikit-learn
    method: str e.g. PCA or LDA
    train/test_data: input data (a list if chosen method is LDA)

    OUTPUT
    ------
    data_reduced: np.array with reduced representation of data

    '''
    if method == "PCA":
        pca = function
        start_time = time.time()
        train_data_reduced = pca.fit_transform(train_data)
        training_time = time.time() - start_time
        test_data_reduced = pca.transform(test_data)
    elif method == "LDA":
        lda = function
        start_time = time.time()
        train_data_reduced = lda.fit_transform(train_data[0], train_data[1])
        training_time = time.time() - start_time
        test_data_reduced = lda.transform(test_data)

    return train_data_reduced, test_data_reduced, training_time

def visuals_2(data: list, method: str):
    '''
    Scatter plot creation to visualize data.

    Parameters
    ----------
    data : list
        reduced data to visualize in a place, along with labels.
    method : str
        name of technique.

    Returns
    -------
    None

    '''
    plt.figure(figsize=(12,6))
    plt.scatter(data[0], data[1], c = data[2],
            edgecolor = 'none', alpha = 0.7, s = 15,
            cmap = plt.cm.get_cmap('nipy_spectral', 10))
    plt.colorbar()
    plt.tight_layout()
    plt.title(f"Fashion-MNIST: {method} projection");

def clustering_technique(function, test_data):
    '''
    Implementing clustering on the test data

    Parameters
    ----------
    function : scikit-learn function
        e.g. DBSCAN(eps = ..., min_samples = ...)
    test_data : numpy array
        normalized and flattened test data

    Returns
    -------
    results of clustering (labels)

    '''
    results = function.fit(test_data)

    return results.labels_

def stacked_ae(data: list):
    '''
    Symmetrical stacked autoencoder architecture and training
    using the provided data

    Parameters
    ----------
    data : list
        train and validation data

    Returns
    -------
    encoder
    reduced_data: numpy array

    '''
    model = Sequential()
    model.add(Dense(512, activation = "relu", input_shape = (784, )))
    model.add(Dense(128, activation = "relu"))
    model.add(Dense(2, activation = "linear", name = "bottleneck"))
    model.add(Dense(128, activation = "relu"))
    model.add(Dense(512, activation = "relu"))
    model.add(Dense(784, activation = "sigmoid"))
    model.compile(loss = "mean_squared_error", optimizer = Adam())
    history = model.fit(data[0], data[0],
                   batch_size = 64, epochs = 10,
                   validation_data = (data[1], data[1]))

    encoder = Model(model.input, model.get_layer("bottleneck").output)
    reduced_data = encoder.predict(data[0])
    reconstruct = model.predict(data[0])

    return encoder, reduced_data, reconstruct

def clustering(model, data, labels):
  '''
  Apply selected clustering techniques.

  Parameters
  ----------
  model: scikit-learn clustering algorithm with specified parameters
  data: numpy array
  labels: numpy array

  Returns
  -------
  results: dict
    Evaluation metrics and xc_time
  pred_labels: array
    results of clustering

  '''
  begin = time.time()
  pred_labels = model.fit_predict(data)
  xc_time = time.time() - begin

  # Evaluation metrics calculation and storing results
  results = {}
  results["Number of suggested clusters"] = len(np.unique(pred_labels))
  results["Execution time of clustering tech. (sec)"] = xc_time
  try:
    ch_score = calinski_harabasz_score(data, pred_labels)
  except:
    ch_score = -999
  results["C-H index"] = ch_score

  try:
    db_score = davies_bouldin_score(data, pred_labels)
  except:
    db_score = -999
  results["D-B index"] = db_score

  try:
    silh_score = silhouette_score(data, pred_labels)
  except:
    silh_score = -999
  results["Silhouette score"] = silh_score

  try:
    ari = adjusted_rand_score(labels, pred_labels)
  except:
    ari = -999
  results["ARI"] = ari

  return results, pred_labels

def visual_3(test_set, true_labels, cluster_labels):
  all_labels = np.unique(true_labels)
  selected = random.sample(list(all_labels), 4)

  fig, axes = plt.subplots(len(selected), 10, figsize = (15, 2*len(selected)))

  for i, label in enumerate(selected):
    row = np.where(cluster_labels == label)[0]
    if len(row) > 0:
      num = np.random.choice(row, size=(10, ))
    else:
      continue

    for k in range(10):
      image = test_set[num[k], ]
      true_l = true_labels[num[k]]
      clust_lab = cluster_labels[num[k]]

      axes[i, k].imshow(image.reshape((28, 28)), cmap = "gray")
      axes[i, k].axis("off")
      axes[i, k].set_title(f"True: {true_l}\nCluster: {clust_lab}", fontsize = 8)

  plt.tight_layout()
  plt.show()

def stacked_ae(data: list):
    '''
    Symmetrical stacked autoencoder architecture and training
    using the provided data

    Parameters
    ----------
    data : list
        train and validation data

    Returns
    -------
    encoder
    reduced_data: numpy array

    '''
    model = Sequential()
    model.add(Dense(512, activation = "relu", input_shape = (784, )))
    model.add(Dense(128, activation = "relu"))
    model.add(Dense(2, activation = "linear", name = "bottleneck"))
    model.add(Dense(128, activation = "relu"))
    model.add(Dense(512, activation = "relu"))
    model.add(Dense(784, activation = "sigmoid"))

    # Explicitly build the model before accessing its input/output for sub-models
    model.build((None, 784)) # Fix: Added explicit build step

    model.compile(loss = "mean_squared_error", optimizer = Adam())
    history = model.fit(data[0], data[0],
                   batch_size = 64, epochs = 10,
                   validation_data = (data[1], data[1]))

    encoder = Model(model.input, model.get_layer("bottleneck").output)
    reduced_data = encoder.predict(data[0])
    reconstruct = model.predict(data[0])

    return encoder, reduced_data, reconstruct

#---------------- M A I N -------------------

# Set seed for reproducibility
SEED = 1312
np.random.seed(SEED)
random.seed(SEED)

# Load Data set
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Generating the validation test
x_train, x_val, y_train, y_val = train_test_split(x_train,
                                                  y_train,
                                                  test_size = 0.16,
                                                  random_state = SEED)

# Dictionary that matches labels to descriptions
labels = {
    0: "T-shirt/top",
    1: "Trouser",
    2: "Pullover",
    3: "Dress",
    4: "Coat",
    5: "Sandal",
    6: "Shirt",
    7: "Sneaker",
    8: "Bag",
    9: "Ankle Boot"
}

# Normalizing and reshaping data
set_dict = {"train": [x_train],
            "val": [x_val],
           "test": [x_test]}

for _ in set_dict.keys():
    set_dict[_].append(data_preprocessing(set_dict[_][0]))


# Dictionary of dimensionality reduction techniques
dim_reduct = {
    "raw": None,
    "PCA": PCA(n_components = 187, whiten = True),
    "LDA": LinearDiscriminantAnalysis(solver = "svd"),
    "SAE": None}

# Dictionary of clustering techniques
cluster_algorithms = {
    "MBKMeans": MiniBatchKMeans(n_clusters = 10, random_state = SEED),
    "DBSCAN": DBSCAN(eps = 0.5, min_samples = 5),
    "Agglomerative Clustering": AgglomerativeClustering(n_clusters = 10)
    }

# Data frame of final results
results_df = pd.DataFrame(columns = ["Dimensionality reduction technique name",
                                     "Clustering algorithm",
                                     "Training time of dim. red. tech. (sec)",
                                     "Execution time of clustering tech. (sec)",
                                     "Number of suggested clusters",
                                     "C-H index",
                                     "D-B index",
                                     "Silhouette score",
                                     "ARI"])

for dim_algorithm in dim_reduct.keys():
    print("Dimensionality technique examined:", dim_algorithm)
    # Update data frame of final results with the dim. reduction tech. name
    results_dict = {"Dimensionality reduction technique name": dim_algorithm}
    if dim_algorithm == "raw":
      results_dict.update({"Training time of dim. red. tech. (sec)": 0})
    # Print random original images for each class
    indices = index_generator(set_dict["train"][1], y_train)
    visuals_1(set_dict["train"][1], y_train, indices,
              "Original images from raw data")

    if dim_algorithm == "PCA":
        # Apply PCA or LDA on train/test sets.
        x_train_trans, x_test_trans, tr_time = dim_reduction(dim_reduct[dim_algorithm],
                                                             dim_algorithm,
                                                             set_dict["train"][1],
                                                             set_dict["test"][1])
        results_dict.update({"Training time of dim. red. tech. (sec)": tr_time})
        # Scatter plot
        visuals_2([x_train_trans[:, 0], x_train_trans[:, 1], y_train], dim_algorithm)
    elif dim_algorithm == "LDA":
        x_train_trans, x_test_trans, tr_time = dim_reduction(dim_reduct[dim_algorithm],
                                                             dim_algorithm,
                                                             [set_dict["train"][1],
                                                              y_train],
                                                             set_dict["test"][1])
        results_dict.update({"Training time of dim. red. tech. (sec)": tr_time})
        # Scatter plot
        visuals_2([x_train_trans[:, 0], x_train_trans[:, 1], y_train], dim_algorithm)
    elif dim_algorithm == "SAE":
        # Train the autoencoder
        start_time = time.time()
        st_encoder, x_train_trans, reconstructed_img = stacked_ae([set_dict["train"][1],
                                               set_dict["val"][1]])
        tr_time = time.time() - start_time
        results_dict.update({"Training time of dim. red. tech. (sec)": tr_time})
        # Reconstructed images for each class
        visuals_1(reconstructed_img, y_train, indices,
                  "Reconstructed images from SAE projection")
        # Scatter plot
        visuals_2([x_train_trans[:, 0], x_train_trans[:, 1], y_train], dim_algorithm)
        # Apply dimensionality reduction technique to test set
        x_test_trans = st_encoder.predict(set_dict["test"][1])

    for clust_algo in cluster_algorithms.keys():
        print("Clustering technique implemented:", clust_algo)
        if dim_algorithm == "raw":
          test_set_x = set_dict["test"][1]
        else:
          test_set_x = x_test_trans
        # Update dictionary of final results
        results_dict.update({"Clustering algorithm": clust_algo})
        # Apply clustering technique on the test data
        cl_results, y_pred = clustering(cluster_algorithms[clust_algo],
                                        test_set_x, y_test)
        # Update dictionary
        results_dict.update(cl_results)

        # Update results data frame
        results_df = pd.concat([results_df, pd.DataFrame([results_dict])], ignore_index=True)

        # Visualizing images that correspond to 4 randomly chosen labels
        visual_3(set_dict["test"][1], y_test, y_pred)

# Store results
results_df.to_csv("Evaluation.csv", index = False)